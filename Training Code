import random 
import json 
import pickle
import numpy as np 
import tensorflow as tf

import nltk
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
intents = json.loads(open(‘intents.json’).read()) # copy & paste the file path of json intents

words = [] classes =[] documents = []
ignoreLetters = [‘?’,’!’,’.’,’,’]

for intent in intents[‘intents’]:
  for pattern in intents[‘patterns’]:
    wordLIst = nltk.word_tokenizer(pattern)
    words.extend(wordList) 
    documents.append((wordLIst, intent[‘tag’])) 
    if intent[‘tag’] not in classes:
      classes.append(intend[‘tag’])


words = [lemmatizer.lemmatize(word) for word in words if word not in ignoreLetters] 
words = sorted(set(classes))

classes = sorted(set(classes))


pickle.dumb(words, open(‘words.pkl’,’wb’)) 
pickle.dump(classes, open(‘classes.pkl’,’wb’))
training = []
outputEmpty = [0]* len(classes)


for document in document:
  bag = []
  wordPatterns = document[0]
  wordPatterns = lemmatizer.lemmatize(word.lower()) for word in wordPatterns]
  for word in words: bag.append(1) if word in wordPatterns else bag.append	(0)

  outputRow = lists(outputEmpty) 
  outputRow[classes.index(document[1])] = 1 training.append(bag + outputRow)

random.shuffle(training) 
training.append(bag+outputRow)

trainX = training[:, len(words)] 
trainY = training[:, len(words)]

model = tf.keras.Sequential()

#Building 3 layers of neural network
model.add(tf.keras.layers.Dense(128.input_shape = (leg(trainX[0]),), activation= ‘relu’)) 
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(64, activation= ‘ relu’)) 
model.add(tf.keras.layers.Dense(len(trainV[0]), activation= ‘ softmax’))

sgd = tf.keras.optimizers.SGD(learninf_rate=0.01, momentum =0.9, nestrova=True) 
model.compile(loss= ‘categorical_crossentropy’, optimizer = sgd, metrics=[‘accuracy’])
Hist = model.fit(np.array(trainX), np.array(trainY), epochs = 200, batch_size = 5, verbose=1) 

model.save(‘chatbot_project.h5’, hist)
print(“Executed”)

